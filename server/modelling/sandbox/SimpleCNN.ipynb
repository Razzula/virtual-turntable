{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 15\n",
      "    Root location: d:\\Programming\\UoM\\virtual-turntable\\server\\modelling\\sandbox\\..\\data\\art\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "['AbbeyRoad_TheBeatles_1969', 'InRainbows_Radiohead_2007', \"JeffWayne'sMusicalVersionofTheWaroftheWorlds_JeffWayne_1978\", 'KidA_Radiohead_2000', 'OKComputer_Radiohead_1997', 'Revolver_TheBeatles_1966', \"Sgt.Pepper'sLonelyHeartsClubBand_TheBeatles_1967\", 'TheDarkSideOfTheMoon_PinkFloyd_1973', 'TheRiseAndFallOfZiggyStardustAndTheSpidersFromMars_DavidBowie_1972', 'TheVelvetUnderground&Nico_TheVelvetUnderground&Nico_1967', 'WishYouWereHere_PinkFloyd_1975']\n",
      "{\"JeffWayne'sMusicalVersionofTheWaroftheWorlds_JeffWayne_1978\": {'name': \"Jeff Wayne's Musical Version of The War of the Worlds\", 'artist': 'Jeff Wayne', 'year': 1978}, 'OKComputer_Radiohead_1997': {'name': 'OK Computer', 'artist': 'Radiohead', 'year': 1997}, 'TheDarkSideOfTheMoon_PinkFloyd_1973': {'name': 'The Dark Side Of The Moon', 'artist': 'Pink Floyd', 'year': 1973}, 'AbbeyRoad_TheBeatles_1969': {'name': 'Abbey Road', 'artist': 'The Beatles', 'year': 1969}, 'Revolver_TheBeatles_1966': {'name': 'Revolver', 'artist': 'The Beatles', 'year': 1966}, 'InRainbows_Radiohead_2007': {'name': 'In Rainbows', 'artist': 'Radiohead', 'year': 2007}, 'KidA_Radiohead_2000': {'name': 'Kid A', 'artist': 'Radiohead', 'year': 2000}, 'WishYouWereHere_PinkFloyd_1975': {'name': 'Wish You Were Here', 'artist': 'Pink Floyd', 'year': 1975}, \"Sgt.Pepper'sLonelyHeartsClubBand_TheBeatles_1967\": {'name': \"Sgt. Pepper's Lonely Hearts Club Band\", 'artist': 'The Beatles', 'year': 1967}, 'TheRiseAndFallOfZiggyStardustAndTheSpidersFromMars_DavidBowie_1972': {'name': 'The Rise And Fall Of Ziggy Stardust And The Spiders From Mars', 'artist': 'David Bowie', 'year': 1972}, 'TheVelvetUnderground&Nico_TheVelvetUnderground&Nico_1967': {'name': 'The Velvet Underground & Nico', 'artist': 'The Velvet Underground & Nico', 'year': 1967}}\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # ensure images are of uniform size\n",
    "    # this is necessary for the neural network to be able to process the images\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # ensure the end result is a tensor\n",
    "    # this converts the image from [0,255] to [0,1]\n",
    "    # making it compatible with the neural network\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # normalise the data\n",
    "    transforms.Normalize(\n",
    "        # this is the mean and standard deviation of the ImageNet dataset\n",
    "        # the pixel value, for each channel, is subtracted by the mean and divided by the standard deviation\n",
    "        # this should shift the pixel values to be centred around zero (mean: 0, std: 1)\n",
    "        # this allows faster and more stable convergence during optimisation\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# LOAD DATASET\n",
    "rootDir = globals()['_dh'][0] # os.path.dirname(os.path.abspath(__file__))\n",
    "dataDir = os.path.join(rootDir, '..', 'data')\n",
    "artDir = os.path.join(dataDir, 'art')\n",
    "\n",
    "dataSet = ImageFolder(root=artDir, transform=transform) # load each subdirectory as a class\n",
    "\n",
    "with open(os.path.join(dataDir, 'manifest.json'), 'r') as f:\n",
    "    trueClasses = json.load(f)\n",
    "\n",
    "print(dataSet)\n",
    "print(dataSet.classes)\n",
    "print(trueClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Data augmentation should be done in the future, to create a more robust dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA\n",
    "trainLoader = DataLoader(dataSet, batch_size=8, shuffle=True)\n",
    "validationLoader = DataLoader(dataSet, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a simple convolutional neural network, for POC.\n",
    "    It has two convolutional layers and two fully connected layers (see below).\n",
    "    It effectively learns the 'ID' of the image (albumName_artistName).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numClasses):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # This is the architecture of the neural network.\n",
    "        # It is composed of two convolutional layers and two fully connected layers.\n",
    "        # conv1 -> pool -> conv2 -> pool -> fc1 -> fc2\n",
    "\n",
    "        # CONVOLUTIONAL LAYER 1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, # 3 channels (RBG)\n",
    "            32, # 32 output filters (feature maps)\n",
    "            kernel_size=3, # each filter will scan 3x3 patches of the image\n",
    "            stride=1, # each filter will move 1 pixel at a time\n",
    "            padding=1 # the input is padded, to esnure the output is the same size as the input\n",
    "        )\n",
    "\n",
    "        # MAX-POOLING LAYER\n",
    "        # reduces spatial dimensions of the input feature maps\n",
    "        # this reduces the number of parameters and computations in the network\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # CONVOLUTIONAL LAYER 2\n",
    "        # the 32 feature maps are now fed into a second layer, resulting in 64 output filters\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # FULLY CONNECTED LAYER 1\n",
    "        self.fc1 = nn.Linear(\n",
    "            64 * 56 * 56, # 64 feature maps, each 56x56 pixels\n",
    "            128 # 128 output neurons\n",
    "        )\n",
    "\n",
    "        # FULLY CONNECTED LAYER 2\n",
    "        # this layer outputs logits (raw scores) for each class\n",
    "        self.fc2 = nn.Linear(128, numClasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "\n",
    "        # pool the first convolutional layer\n",
    "        # the size of the feature maps are halved\n",
    "        x = self.pool(\n",
    "            # apply the ReLU activation function\n",
    "            # this introduces non-linearity to the model\n",
    "            torch.relu(\n",
    "                # apply the first convolutional layer\n",
    "                # the 32 filters are applied to the input image\n",
    "                self.conv1(x)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # pool the second convolutional layer\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "\n",
    "        # flatten the feature maps\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "\n",
    "        # fully-connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # no activation function, as this is handled by the loss function\n",
    "\n",
    "        return x\n",
    "\n",
    "numClasses = len(dataSet.classes)\n",
    "model = SimpleCNN(numClasses=numClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "# used to compute the error between the model's predictions and the true labels\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# OPTIMISER\n",
    "# updates the model's weights, based on gradients\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) Loss: 14.165348410606384\t Accuracy: 0.2\n",
      "(Epoch 2) Loss: 11.008213758468628\t Accuracy: 0.26666666666666666\n",
      "(Epoch 3) Loss: 3.726523458957672\t Accuracy: 0.4666666666666667\n",
      "(Epoch 4) Loss: 0.20623963698744774\t Accuracy: 1.0\n",
      "(Epoch 5) Loss: 0.03746506292372942\t Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def train(model, trainLoader, criterion, optimiser, epochs=5):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        trainLoader (_type_): _description_\n",
    "        criterion (_type_): _description_\n",
    "        optimiser (_type_): _description_\n",
    "        epochs (int, optional): _description_. Defaults to 5.\n",
    "    \"\"\"\n",
    "\n",
    "    # EPOCH LOOP\n",
    "    # an epoch is a complete pass through the dataset\n",
    "    # we do this several times, to allow the model to learn\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train() # set the model to training mode (this is necessary for dropout and batch normalisation)\n",
    "\n",
    "        # initialsie some statistic-trackers\n",
    "        runningLoss = 0.0 # cumulative loss\n",
    "        correct, total = 0, 0 # correct predictions, total predictions\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for images, labels in trainLoader:\n",
    "            optimiser.zero_grad() # prevent accumulated gradients from previous iterations overflowing\n",
    "\n",
    "            # FORWARD PASS\n",
    "            # pass the images into the model, producing a prediction\n",
    "            outputs = model(images)\n",
    "\n",
    "            # COMPUTE LOSS\n",
    "            # compare the model's predictions to the true labels\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # BACKPROPAGATION\n",
    "            # compute the gradients of the loss, with respect to the model's parameters/weights\n",
    "            loss.backward()\n",
    "            # and update the model's weights accordingly\n",
    "            optimiser.step()\n",
    "\n",
    "            # track stats\n",
    "            runningLoss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f'(Epoch {epoch+1}) Loss: {runningLoss/len(trainLoader)}\\t Accuracy: {correct/total}')\n",
    "\n",
    "train(model, trainLoader, criterion, optimiser, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seen data (hopefully, expacted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: JeffWayne'sMusicalVersionofTheWaroftheWorlds_JeffWayne_1978 (0.8505333662033081)\n",
      "{'name': \"Jeff Wayne's Musical Version of The War of the Worlds\", 'artist': 'Jeff Wayne', 'year': 1978}\n"
     ]
    }
   ],
   "source": [
    "testImage = Image.open(os.path.join(rootDir, '..', 'data', 'misc', \"JeffWayne'sMusicalVersionofTheWaroftheWorlds_JeffWayne(2).png\"))\n",
    "testImage = transform(testImage)\n",
    "testImage = testImage.unsqueeze(0) # add batch dimension, as model expects it\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(testImage)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    predictedProb, predictedClass = torch.max(probabilities, 1)\n",
    "\n",
    "    predictedID = dataSet.classes[predictedClass.item()]\n",
    "    print(f'Predicted: {predictedID} ({predictedProb.item()})')\n",
    "    print(trueClasses[predictedID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unseen data (the model is not expected to perform well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: OKComputer_Radiohead_1997 (0.4726549983024597)\n",
      "{'name': 'OK Computer', 'artist': 'Radiohead', 'year': 1997}\n"
     ]
    }
   ],
   "source": [
    "testImage = Image.open(os.path.join(rootDir, '..', 'data', 'misc', 'ABBA_ABBA.png'))\n",
    "testImage = transform(testImage).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(testImage)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    predictedProb, predictedClass = torch.max(probabilities, 1)\n",
    "\n",
    "    predictedID = dataSet.classes[predictedClass.item()]\n",
    "    print(f'Predicted: {predictedID} ({predictedProb.item()})')\n",
    "    print(trueClasses[predictedID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the sum of the probabilities will be 1. Therefore, with a small datatset, the model may be confidently wrong (i.e. a high probability for the wrong class), as this is moreso a measure of 'how confident I am it is this, compared to the other options', as opposed to 'how confident I am that it is this, and not anything else'. This is important to note when interpreting the results (we should have a high standard for confidence). However, as the dataset grows, the liklihood of any one class being highly-favoured, when the true result lies outside of the trained classes, should decrease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-turntable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
